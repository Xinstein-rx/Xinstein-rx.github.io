<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>卷积神经网络(CNN) | Xinsteinのblog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 6.2.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">卷积神经网络(CNN)</h1><a id="logo" href="/.">Xinsteinのblog</a><p class="description">Brainy is the new sexy.</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">卷积神经网络(CNN)</h1><div class="post-meta">2020-06-01</div><div class="post-content"><h3 id="边缘检测示例"><a href="#边缘检测示例" class="headerlink" title="边缘检测示例"></a>边缘检测示例</h3><p>将一张图像以灰度值矩阵的方式展现出来，然后与一个过滤器（这里的过滤器也是一个特殊的矩阵）做卷积运算，类似于一个滑动窗口，就能得到一个新的矩阵，这个新的矩阵亮度大的地方就是原图片边缘的地方。具体过程如图1</p>
<div align = center>
    <img src = "卷积神经网络/16.PNG"><br><br>
    Fig1. 卷积边缘检测
</div>

<blockquote>
<p>卷积运算提供了一个方便的发现图像中垂直边缘的例子，要想计算出水平边缘，将过滤器转置一下就可以了。里面数字的选择也是不同的，有很多版本。卷积神经网络，就是将这九个数看成参数，利用反向传播算法，学习出这些参数的值，就可以检测更复杂的边缘了，不仅仅是垂直水平边缘。</p>
</blockquote>
<span id="more"></span>

<h3 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h3><h4 id="padding"><a href="#padding" class="headerlink" title="padding"></a><strong>padding</strong></h4><p>之所以会有 padding（就是对图片的填充），有两个原因：</p>
<ul>
<li>每做完一次卷积之后，图片就会变小，要是网络层数很多的话，就会导致图片变得非常小，这是我们不希望的</li>
<li>边缘的像素点的作用会非常弱，也就是说，边缘像素点在卷积计算中用到的次数不多。</li>
</ul>
<blockquote>
<p>Vaild 和 Same 卷积：</p>
<ol>
<li>Vaild 卷积就是不对图片进行 padding</li>
<li>Same 卷积就是，进行卷积运算之后的图片和原图片大小相同，这就要求 padiing的$p &#x3D; \frac{f-1}{2}$</li>
<li>一般来说，过滤器的大小 f 是一个奇数，很少有偶数</li>
</ol>
</blockquote>
<h4 id="卷积步长"><a href="#卷积步长" class="headerlink" title="卷积步长"></a>卷积步长</h4><p>步长的选择会影响到卷积之后图片的大小，假设有一个 $n<em>n$的图片，过滤器大小是$f</em>f$,padding 为 <em>p</em>, 步长是 <em>s</em>, 则卷积之后的图片大小为<br>$$<br>\lfloor \dfrac{n+2p-f}{s}+1 \rfloor \times \lfloor \dfrac{n+2p-f}{s}+1 \rfloor<br>$$<br>严格来讲，真正的卷积操作，是将过滤器先按照副对角线翻折，在进行乘积求和，但是在深度学习中并没有这样做，但是我们仍然叫他卷积操作，尽管数学家们称之为相关。</p>
<h4 id="立体卷积"><a href="#立体卷积" class="headerlink" title="立体卷积"></a>立体卷积</h4><p>立体卷积就是对一幅 RGB 图像进行卷积，而不是仅仅的灰度图像了。所用的过滤器也变成了和图片通道数相同的立体性形状，但是最后得出的结果却是一个二维的图形。要是用了多个过滤器，如想要实现垂直检测和水平检测，那输出的通道数应该和过滤器的个数是相同的。如图2</p>
<div align = center>
    <img src = "卷积神经网络/17.PNG"><br><br>
    Fig2. 立体卷积
</div>

<h4 id="单层卷积网络"><a href="#单层卷积网络" class="headerlink" title="单层卷积网络"></a>单层卷积网络</h4><p>卷积网络前向传播的过程是：</p>
<ol>
<li>设置几个过滤器，每个过滤器用来提取不同的特征，如水平特征，垂直特征，过滤器类似于权重，符号化表示为$w^{[l]}$</li>
<li>将图片与过滤器分别做卷积$z^{[l]}&#x3D;w^{[l]}a^{[l-1]}$</li>
<li>将卷积之后的结果加上一个偏差值$b_j$即$z^{[l]}&#x3D;w^{[l]}a^{[l-1]}+b_j$</li>
<li>再将最后的结果输入到一个激活函数Relu函数$output&#x3D;Relu(z^{[l]})$</li>
<li>将最后得到的三维矩阵展开成一个向量，再做Logistic回归或者是softmax回归</li>
</ol>
<blockquote>
<p>通过这种设置过滤器的方法，使得参数只是由过滤器决定，而不再由图片的大小决定，就避免了图片大，参数多，而过拟合的现象。</p>
<p>将最后卷积网络输出的结果，即一个三维的矩阵，展开成一个向量，这些数字就是提取的特征，将这些特征输入到一个 Logistic 回归或者是 softmax 回归中，就可以做分类了。Logistic 回归是做二元分类，softmax 回归是做多元分类。</p>
</blockquote>
<h4 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h4><p>运用池化层的目的是，缩减模型大小，提高计算速度，提高所取特征的鲁棒性。池化分为两种，最大池化和平均池化。最大池化的意思是只要我在某个象限（也可以说是某个区域），发现了具有这个特征（每一个参数都代表一个特征，数字大就说明这个特征明显），就保留他的最大值，缩小模型，这个方法非常的有效，尽管可能不是很理解为什么这个样子，就可以做到高效。平均池化顾名思义，就是求每一个区域的平均值。下面是一个平均池化的例子（图3）</p>
<div align = center>
    <img src = "卷积神经网络/18.PNG"><br><br>
    Fig3. 平均池化
</div>

<p>池化过程和卷积过程是类似的，那最大池化来说，也是选择一个过滤器和图片做卷积，只不过不再需要线性相乘再相加，直接找出最大值就好，因此它不需要去学习什么参数。立体卷积的化，就每一个 channel 做池化运算，最后得出的结果也是立体的矩阵。</p>
<p>一般来说，池化的参数 filter 的大小是 2*2，步长 stride 是 2，这是最常用的。</p>
<h4 id="卷积网络案例"><a href="#卷积网络案例" class="headerlink" title="卷积网络案例"></a>卷积网络案例</h4><blockquote>
<p>一般的卷积网络架构是这样的，<em>Conv</em>1 <em>→</em> <em>P ool</em> <em>→</em> <em>Conv</em>2 <em>→</em> <em>P ool</em> <em>→</em> <em>F C</em> <em>→</em> <em>F c</em> <em>→</em> <em>F C</em> <em>→</em> <em>sof tM ax</em></p>
</blockquote>
<p>具体过程图4所示</p>
<div align = center>
    <img src = "卷积神经网络/19.PNG"><br><br>
    Fig4. 卷积神经网络示例
</div>

<p>一般来讲，这些池化参数的确定最好不要自己设置，而是阅读大量文献，看看那些成功的案例都是怎么设置参数的，也可以直接把里面的参数应用到自己的案例当中。</p>
<h4 id="为什么使用卷积"><a href="#为什么使用卷积" class="headerlink" title="为什么使用卷积"></a>为什么使用卷积</h4><p>和只用全连接相比，卷积可以做到参数共享和稀疏连接</p>
<blockquote>
<p>参数共享：参数共享的意思就是，将一个过滤器应用在图片的某个地方，可以很好的提取特征，同样的，在图片的其他地方，应用这个过滤器中的参数，同样可以做到提取特征，不需要设置新的参数。</p>
</blockquote>
<blockquote>
<p>稀疏连接：卷积之后得到的矩阵中的每一个数字，只与原来图片中的某些像素连接，而不是和所有像素全连接。</p>
</blockquote>
<div align = center>
    <img src = "卷积神经网络/20.PNG"><br><br>
    Fig5. 参数共享和稀疏连接
</div>

<p>通过这些办法，就可以减少参数的数量，一是加快了计算速度，二是避免了过拟合问题。</p>
<h3 id="经典卷积神经网络"><a href="#经典卷积神经网络" class="headerlink" title="经典卷积神经网络"></a>经典卷积神经网络</h3><h4 id="LeNet-5-网络"><a href="#LeNet-5-网络" class="headerlink" title="LeNet-5 网络"></a><strong>LeNet-5</strong> 网络</h4><p>LeNet-5 网络主要用来处理灰度图像。这种网络的大致思想仍然被现在科学家们所使用，就是一个或多个卷积层后面跟着一个池化层，再跟着一个或多个卷积层，然后再跟着一个池化层，最后是全连接层。网络结构如图6，注意过滤器大小和步长的选取。</p>
<div align = center>
    <img src = "卷积神经网络/21.PNG"><br><br>
    Fig6. LeNet-5 网络
</div>

<h4 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a><strong>AlexNet</strong></h4><p>AlexNet 网络与 LeNet-5 网络相比，AlexNet 网络更加复杂，但是应用了 Relu激活函数 LeNet-5 用的激活函数是 sigmoid 函数。AlexNet 网络如图7</p>
<div align = center>
    <img src = "卷积神经网络/22.PNG"><br><br>
    Fig7. AlexNet 网络
</div>

<h4 id="VGG-16"><a href="#VGG-16" class="headerlink" title="VGG-16"></a><strong>VGG-16</strong></h4><p>这是一种只需要专注于构建卷积层的简单网络，16 这个数字指的是在这个网络包含 16个卷积层和全连接层。这是一个很大的网络，总共包含约 1.38 亿个参数。但是网络并不复杂，而且比较规整，都是几个卷积层后面跟着可以压缩图像的池化层。同时，卷积层过滤器数量存在着一定的规律性，具体结构如图8</p>
<p>上述图片并没有把所有的卷积层都画出来，对于那些操作相同的网路就合并了。	</p>
<div align = center>
    <img src = "卷积神经网络/23.PNG"><br><br>
    Fig8. VGG-16 网络
</div>

<h3 id="残差网络"><a href="#残差网络" class="headerlink" title="残差网络"></a>残差网络</h3><p>一般来说，如果是网络的层数太多，就会存在梯度消失和梯度爆炸的问题。</p>
<blockquote>
<p><strong>Definition 11.1</strong> <em>skip connection&#x2F;short cut</em>: 将某一层的激活值，迅速传递到更深的层中去。通过这种方法，就可以建立 ResNet 训练更深的网络。</p>
</blockquote>
<blockquote>
<p>skip connection 的一个直观理解，就是尽量避免因为神经网络层数过多，而产生一种损失原信息的效应，通过这种把原信息往后传播的方法，可以尽量弥补在进行线性或者非线性变化过程中，损失的特征信息，从而可以训练更深的网络。ResNet 网络的构建方法就是把许多残差块堆积在一起，形成一个深度神经网络。</p>
</blockquote>
<div align = center>
    <img src = "卷积神经网络/24.PNG"><br><br>
    Fig9. 残差块
</div>

<div align = center>
    <img src = "卷积神经网络/25.PNG"><br><br>
    Fig10. ResNet
</div>

<p>理论上说，网络的层数越深，错误应该越少，但是在实际的应用中，随着网络层数的加深，错误先减少后增加，这是在没有 skip connection 的情况下，有了 skip connection 无论网络的层数有多少，效果都是不错的。两种方式见图11和图12</p>
<div align = center>
    <img src = "卷积神经网络/26.PNG"><br><br>
    Fig11. Plain
</div>

<div align = center>
    <img src = "卷积神经网络/27.PNG"><br><br>
    Fig12. ResNet
</div>

<h3 id="1-1-卷积"><a href="#1-1-卷积" class="headerlink" title="1*1 卷积"></a><strong>1*1</strong> 卷积</h3><blockquote>
<p>1*1 卷积就是过滤器的大小是 1*1 的，对于一个二维的图像来说，使用 1*1 过滤器也仅仅是把每个像素放大了，看似没什么用。但是到了一个三维的图片，利用 1*1过滤器，就相当于每个切片和过滤器做了一个全连接，再用 Relu 做一个非线性变换。其实讲到这里我还是不是很明白，这个 1*1 的卷积到底能用来干什么。视频中讲了一个用处就是用来压缩图片的 channel 数量。我们知道，利用池化操作，可以减少图片的宽度和高度，不能减少 channel，而 1*1 卷积则提供了这样一种方法，可以压缩channel 的数量，当然也可以增加 channel 的数量。当然了，在这个增加 channel 的过程中，并不是简单的增加数量，也对原来的结果做了一次非线性的变换，再增加channel。压缩 channel 的过程如图13</p>
</blockquote>
<div align = center>
    <img src = "卷积神经网络/28.PNG"><br><br>
    Fig13. channel 压缩
</div>

<h3 id="inception-网络"><a href="#inception-网络" class="headerlink" title="inception 网络"></a><strong>inception</strong> 网络</h3><h4 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h4><p>inception 网络的作用就是，代替人工来确定卷积层中的过滤器类型，或者是否需要创建卷积层或池化层。</p>
<blockquote>
<p>当你不确定需要什么样的过滤器或者是是否需要池化层的时候，就可以利用inception 网络，把所有过滤器都应用一遍，然后把结果都连接起来，神经网络会自己学习需要什么样的过滤器以及是否需要池化层。具体过程如图14</p>
</blockquote>
<div align = center>
    <img src = "卷积神经网络/29.PNG"><br><br>
    Fig14. inception 网络原理
</div>

<p>神经网络还有一个问题就是计算的代价，有时需要缩小计算代价，就要用一个1*1 的卷积神经网络，来缩小 channel 的值，减少计算量。做完 1*1 卷积之后的结果叫做瓶颈，这里并不是困难的意思，而是说一个瓶子当中最小的部分。事实证明，只要合理选取瓶颈的大小，并不会影响神经网络的性能。如15图示</p>
<div align = center>
    <img src = "卷积神经网络/30.PNG"><br><br>
    Fig15. 缩小 channel
</div>

<h4 id="具体实例"><a href="#具体实例" class="headerlink" title="具体实例"></a>具体实例</h4><p>通常 inception 网络是放在一个神经网络的中间部分的，并不是整个网络都是用inception 网络。下面来看一下一个 inception module 的例子，图16</p>
<div align = center>
    <img src = "卷积神经网络/31.PNG"><br><br>
    Fig16. inception module
</div>

<blockquote>
<p>在讲的时候，我就有个疑惑，既然 3<em>3 和 5</em>5 的过滤器都需要经过一个 1*1 的卷积，那为啥不把这一步合并一下，经过一个 1*1 卷积，在进行两个 3*3 和 5*5 呢，原来是他们用的 1*1 卷积不相同。经过一个池化层，只能缩小高度和宽度，不能缩小 channel，这样就导致，channel 数量太多，因此，为了减少计算量，在池化层后面还会加一个 1*1 的卷积层，来缩小 channel。有个疑惑的问题搞不太懂，就是既然inception 网络，能自己决定用 3*3 过滤器还是 5*5 过滤器，那个这些过滤器的个数是怎么确定的呢，我看好像还是认为确定的 emmmm，难搞哦。</p>
</blockquote>
<p>有了基本的 inception module，就可以建立一个有 inception 网络的神经网络，如图17</p>
<div align = center>
    <img src = "卷积神经网络/32.PNG"><br><br>
    Fig17.  inception 网络
</div>

<blockquote>
<p>图17这个网络，是由许多的 inception module 组成的，有的是直接利用前一层的激活值，有的是先把激活值做一次池化，缩小一下规模，在输入到一个 inception module 中。有个值得注意的细节就是，这个神经网络在中间也做了预测的输出，就是利用中间层的值，来预测结果。我想这样做的原因是，是为了防止层数过深而过拟合吧，就是他们也不确定这个层数是不是一个比较好的层数，于是就在中间层输出一下，看看结果。有个有趣的梗就是，inception 这个名字的由来是引自盗梦空间，表示科学家们立志要做出更深层次的网络。</p>
</blockquote>
<h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><p>有时候我们需要对数据进行一些变换，增加我们网络的鲁棒性。具体的一些操作方法如下</p>
<div align = center>
    <img src = "卷积神经网络/33.PNG"><br><br>
    Fig18.  Mirroring
</div>

<div align = center>
    <img src = "卷积神经网络/34.PNG"><br><br>
    Fig19. Random Cropping
</div>

<div align = center>
    <img src = "卷积神经网络/35.PNG"><br><br>
    Fig20.  Color Shifiting
</div></div><div class="tags"><a href="/tags/深度学习"><i class="fa fa-tag">深度学习</i></a><a href="/tags/CNN"><i class="fa fa-tag">CNN</i></a></div><div class="post-nav"><a class="pre" href="/2020/06/03/1125-Chain-the-Ropes-25%E5%88%86/">1125 Chain the Ropes (25分)</a><a class="next" href="/2020/06/01/1133-Splitting-A-Linked-List-25%E5%88%86/">1133 Splitting A Linked List (25分)</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://yoursite.com"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="About"><img class="nofancybox" src="/img/avatar.png"/></a><p>To be a better man.</p><a class="info-icon" href="https://twitter.com/username" title="Twitter" target="_blank" style="margin-inline:5px"> <i class="fa fa-twitter-square" style="margin-inline:5px"></i></a><a class="info-icon" href="mailto:admin@domain.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/username" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a><a class="info-icon" href="/atom.xml" title="RSS" target="_blank" style="margin-inline:5px"> <i class="fa fa-rss-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/PAT-Advanced/" style="font-size: 15px;">PAT-Advanced</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/dfs/" style="font-size: 15px;">dfs</a> <a href="/tags/%E4%BA%8C%E5%88%86/" style="font-size: 15px;">二分</a> <a href="/tags/%E5%9B%BE%E8%AE%BA/" style="font-size: 15px;">图论</a> <a href="/tags/%E8%BF%9E%E9%80%9A%E5%88%86%E9%87%8F/" style="font-size: 15px;">连通分量</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">排序</a> <a href="/tags/%E6%A8%A1%E6%8B%9F/" style="font-size: 15px;">模拟</a> <a href="/tags/map/" style="font-size: 15px;">map</a> <a href="/tags/%E6%A0%91%E7%9A%84%E7%9B%B4%E5%BE%84/" style="font-size: 15px;">树的直径</a> <a href="/tags/%E9%9B%86%E5%90%88/" style="font-size: 15px;">集合</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15px;">链表</a> <a href="/tags/%E6%80%9D%E7%BB%B4/" style="font-size: 15px;">思维</a> <a href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" style="font-size: 15px;">动态规划</a> <a href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/" style="font-size: 15px;">字符串</a> <a href="/tags/%E6%99%BA%E5%8A%9B%E9%A2%98/" style="font-size: 15px;">智力题</a> <a href="/tags/%E5%BF%AB%E6%8E%92/" style="font-size: 15px;">快排</a> <a href="/tags/%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">插入排序</a> <a href="/tags/%E5%A0%86/" style="font-size: 15px;">堆</a> <a href="/tags/%E5%B9%B6%E6%9F%A5%E9%9B%86/" style="font-size: 15px;">并查集</a> <a href="/tags/%E4%BC%98%E5%85%88%E9%98%9F%E5%88%97/" style="font-size: 15px;">优先队列</a> <a href="/tags/%E6%95%A3%E5%88%97/" style="font-size: 15px;">散列</a> <a href="/tags/%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">拓扑排序</a> <a href="/tags/Git/" style="font-size: 15px;">Git</a> <a href="/tags/%E8%A7%84%E8%8C%83/" style="font-size: 15px;">规范</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">机器学习</a> <a href="/tags/%E6%9C%80%E7%9F%AD%E8%B7%AF/" style="font-size: 15px;">最短路</a> <a href="/tags/Dijkstra/" style="font-size: 15px;">Dijkstra</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" style="font-size: 15px;">数据库</a> <a href="/tags/mongodb/" style="font-size: 15px;">mongodb</a> <a href="/tags/elasticsearch/" style="font-size: 15px;">elasticsearch</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Tableau%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/" style="font-size: 15px;">Tableau数据分析</a> <a href="/tags/cookie/" style="font-size: 15px;">cookie</a> <a href="/tags/session/" style="font-size: 15px;">session</a> <a href="/tags/UML/" style="font-size: 15px;">UML</a> <a href="/tags/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/" style="font-size: 15px;">软件工程</a> <a href="/tags/servlet/" style="font-size: 15px;">servlet</a> <a href="/tags/OS/" style="font-size: 15px;">OS</a> <a href="/tags/%E5%B0%8F%E7%8E%A9%E5%85%B7/" style="font-size: 15px;">小玩具</a> <a href="/tags/%E5%9B%9E%E6%BA%AF/" style="font-size: 15px;">回溯</a> <a href="/tags/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/" style="font-size: 15px;">贪心算法</a> <a href="/tags/%E5%8C%BA%E9%97%B4%E9%97%AE%E9%A2%98/" style="font-size: 15px;">区间问题</a> <a href="/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/" style="font-size: 15px;">二叉树</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">深度学习</a> <a href="/tags/%E5%BE%AA%E7%8E%AF%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">循环序列模型</a> <a href="/tags/CNN/" style="font-size: 15px;">CNN</a> <a href="/tags/%E7%99%BD%E7%9B%92%E6%B5%8B%E8%AF%95/" style="font-size: 15px;">白盒测试</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">目标检测</a> <a href="/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/" style="font-size: 15px;">论文研读</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" style="font-size: 15px;">计算机视觉</a> <a href="/tags/STL/" style="font-size: 15px;">STL</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2022/12/24/C-STL%E4%B9%8Bupper-bound%E5%92%8Clower-bound/">C++STL之lower_bound & upper_bound</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/12/14/%E3%80%90OS%E3%80%91%E7%AC%AC%E4%B8%89%E5%A4%A9-%E8%BF%9B%E5%85%A532%E4%BD%8D%E6%A8%A1%E5%BC%8F%E5%B9%B6%E5%AF%BC%E5%85%A5c%E8%AF%AD%E8%A8%80/">【OS】第三天 进入32位模式并导入c语言</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/12/14/%E3%80%90OS%E3%80%91%E7%AC%AC%E4%BA%8C%E5%A4%A9-%E6%B1%87%E7%BC%96%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%E4%B8%8Emakefile%E5%85%A5%E9%97%A8/">【OS】第二天 汇编语言学习与makefile入门</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/12/14/%E3%80%90OS%E3%80%91%E7%AC%AC%E4%B8%80%E5%A4%A9-%E4%BB%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%93%E6%9E%84%E5%88%B0%E6%B1%87%E7%BC%96%E7%A8%8B%E5%BA%8F%E5%85%A5%E9%97%A8/">【OS】第一天 从计算机结构到汇编程序入门</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/29/%E3%80%90%E9%93%BE%E8%A1%A8%E6%80%BB%E7%BB%93%E3%80%91%E5%BF%AB%E6%85%A2%E6%8C%87%E9%92%88%E5%B0%B1%E6%98%AF%E7%89%9B%EF%BD%9E/">【链表总结】快慢指针就是牛～</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/29/%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E9%9D%9E%E9%80%92%E5%BD%92%E9%81%8D%E5%8E%86/">二叉树的非递归遍历</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/25/%E3%80%90%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E3%80%91%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%E4%BA%8C%EF%BC%9A%E5%8C%BA%E9%97%B4%E9%97%AE%E9%A2%98/">【贪心算法】贪心算法系列二：区间问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/25/%E3%80%90%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E3%80%91%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%E4%B8%80/">【贪心算法】贪心算法系列一</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/07/03/%E3%80%90%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95%E3%80%91%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/">【回溯算法】回溯算法总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/06/21/%E3%80%90%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E3%80%91%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%80%BB%E7%BB%93/">【动态规划】动态规划总结</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Links</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2022 <a href="/." rel="nofollow">Xinsteinのblog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="Copy Successed!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>