<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>神经网络 | Xinsteinのblog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 6.2.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">神经网络</h1><a id="logo" href="/.">Xinsteinのblog</a><p class="description">Brainy is the new sexy.</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">神经网络</h1><div class="post-meta">2020-05-25</div><div class="post-content"><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><h4 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h4><p>我目前对于神经网络的理解，就是他是好几个 Logistic 回归符合起来的，然后一层一个 Logistic 回归，输出还可以有多个。</p>
<p>神经网络的代价函数有点复杂，复杂到我不太想写…… 就是这个魔鬼的函数<br>$$<br>J(\theta)&#x3D;-\dfrac{1}{m}[\sum_{i&#x3D;1}^{m}\sum_{k&#x3D;1}^{K}y^{(i)}<em>klog(h_\theta(x^{(i)}))<em>k+(1-y^{(i)}<em>k)log(1-(h_\theta(x^{(i)}))<em>k)]+\frac{\lambda}{2m}\sum</em>{l&#x3D;1}^{L-1}\sum</em>{i&#x3D;1}^{s_l}\sum</em>{j&#x3D;1}^{s</em>{l+1}}(\theta^{l}_{ij})^2<br>$$<br>啊好长…….</p>
<span id="more"></span>

<h4 id="代价函数-1"><a href="#代价函数-1" class="headerlink" title="代价函数"></a>代价函数</h4><p>我们知道了神经网络的代价函数，就是对所有的输出单元与样本值的代价之和，然后正则项也是对所有参数的惩罚，下面要做的就是对这个代价函数最小化，利用梯度下降算法或者是其他更高级的算法。想要利用梯度下降算法就要使用偏导数，因此就要求偏导数。</p>
<blockquote>
<p>对偏导数的求法，就是利用反向传播来做的。反向传播就是把最后一层的误差往前传到前一层，而恰恰就是这个误差的积累，就把偏导数算出来了…… 这非常的amazing，我不能很懂嘤嘤嘤。</p>
</blockquote>
<p>这个误差的算法是这样的：</p>
<p>对于最后一层的误差（假设这个神经网络有四层）：<br>$$<br>\delta^{(4)}_j&#x3D;a^{(i)}_j-y_j<br>$$<br>然后对于前一层，将这个误差向前传播得到：<br>$$<br>\delta^{(3)}&#x3D;(\theta^{(3)})^T\delta^{(4)}.*g’(z^{(3)})<br>$$</p>
<p>$$<br>\delta^{(2)}&#x3D;(\theta^{(2)})^T\delta^{(3)}.*g’(z^{(2)})<br>$$</p>
<p>上面的导数项经过推到可以得到<br>$$<br>g’(z^{(3)})&#x3D;a^{(3)}.*(1-a^{(3)})<br>$$<br>那么算这些误差有什么用呢， 经过复杂的数学推导，在不是很严谨的情况下，我们可以说<br>$$<br>\dfrac{\delta}{\delta \theta^{(l)}_{ij}}J(\theta)&#x3D;a^{(l)}_j\delta^{(l+1)}_i<br>$$<br>这样我们就通过这个误差算出了所有参数的偏导数项，进而就能用梯度下降了。观察下这个式子，就是每一层每个单元的激活值，乘上下一层对应单元的误差值。</p>
<p>总结一下这个过程：</p>
<blockquote>
<p><strong>Definition 1</strong>  反向传播算法</p>
<p>训练集为${(x^{(i)},y^{(i)}),…,(x^{(m)},y^{(m)})}$</p>
<p>set $\Delta^{(l)}_{ij}&#x3D;0<del>(for</del>all~l,i,j)$</p>
<p>For i&#x3D;1 to m</p>
<p>set $a^{(1)}&#x3D;x^{(i)}$</p>
<p>利用前向传播计算出每一层每一个单元的激活值</p>
<p>从最后一层开始前向传播计算$\delta^{(l)}_{ij}$的值</p>
<p>最后累加$\Delta$即：<br>$$<br>\Delta^{(l)}<em>{ij}:&#x3D;\Delta^{(l)}</em>{ij}+a^{(l)}<em>j\delta^{(l+1)}<em>i<br>$$<br>最后得到计算偏导数的方法：<br>$$<br>\left{<br>        \begin{array}{ll}<br>        D^{(l)}</em>{ij}:&#x3D;\frac{1}{m}\Delta^{(l)}</em>{ij}+\lambda\theta^{(l)}<em>{ij} &amp; \text{if } j \ne 0,\<br>        D^{(l)}</em>{ij}:&#x3D;\frac{1}{m}\Delta^{(l)}<em>{ij} &amp; \text{if } j &#x3D; 0<br>        \end{array} \right.<br>$$<br>这里的$\Delta$是用来计算偏导数的，是一个累计误差值<br>$$<br>\dfrac{\delta}{\delta \theta^{(l)}</em>{ij}}J(\theta)&#x3D;D^{(l)}_{ij}<br>$$</p>
</blockquote>
<h4 id="理解反向传播"><a href="#理解反向传播" class="headerlink" title="理解反向传播"></a>理解反向传播</h4><p>反向传播算法其实就是在想方设法算一个偏导数，最开始我们有一个复杂的公式来前向传播每一个误差项，但是其实有更简单的方法，如下图1</p>
<div align = center>
    <img src = 神经网络/14.PNG> <br><br>
    FIG1. 理解反向传播
</div>

<p>从图上来看，计算的每一个单元的 $ \delta^{(l)}_j $项，就是对当前单元激活值求的偏导数（其实不太懂，可能是微积分学的不到位。。。），然后计算这些误差项的步骤不用再是那么麻烦的套用公式了，直接就是后一层误差项的加权和。</p>
<h4 id="梯度检测"><a href="#梯度检测" class="headerlink" title="梯度检测"></a>梯度检测</h4><p>梯度检测是为了检查我的反向传播算法是不是正确的，因为很多的错误都是因为反向传播引起的。</p>
<p>概括地来说梯度检测，就是用割线去近似那一点的切线（就是导数的数学定义），然后算出那一点的偏导数，再用反向传播得到的偏导数和计算出的近似偏导数作比较，如果两者近似相等，就说明反向传播算法是正确的，具体计算偏导数的过程如图2</p>
<div align = center>
    <img src = 神经网络/15.PNG> <br><br>
    FIG1. 计算偏导数
</div>

<blockquote>
<p>还要注意的一点就是如果证明了反向传播算法是正确的，在进行训练的时候，就要关闭梯度检测的代码，因为梯度检测是个很慢的算法，一直检测的话会导致效率不高。</p>
<p>写代码的时候一般是把矩阵形式的参数，展开成一个 n 维的向量，包括偏导数也是一个 n 维的向量，便于写代码。<em>ϵ</em> 一般来说，不会取的很小，因为会发生一些数值问题。</p>
</blockquote>
<h4 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h4><p>随机初始化的意思就是，把参数的值随机初始化到一个 [- <em>ϵ, ϵ</em>] 的区间里，使得每个参数都很接近 0，但不能所有的参数一开始都初始化为 0，这样会导致神经网络出现对称参数的问题，即所有相关的参数的更新都会以相同的方式，对于每一组参数，计算出的偏导数也相同（不同组不相同），使得网络出现大量冗余。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>网络结构选取时要注意</p>
<ol>
<li>一般来讲，只有一个隐藏层的网络是比较常见的</li>
<li>要是想使用多个隐藏层的网络的话，建议隐藏层节点的数目都是相同的</li>
<li>隐藏层单元的数目一般来说是越多越好，但是太多的话容易导致网络变慢，隐藏层单元的选择还要和输入匹配，可以和输入的维度相同，也可以是他的两倍，三倍等</li>
</ol>
<p>神经网络的一般步骤</p>
<ol>
<li>构建一个神经网络，然后初始化权重。通常把权重设置为很小的值，接近于零</li>
<li>根据前向传播计算$h_\theta(x^{(i)})$</li>
<li>计算$J(\theta)$</li>
<li>利用反向传播，计算偏导数$\dfrac{\delta}{\delta \theta^{(l)}_{ij}}J(\theta)$</li>
<li>进行梯度检测，比较反向传播得到的偏导数值，和数值方法得到的偏导数值，来保证反向传播算法的正确性。检查完之后记得关闭梯度检测。</li>
<li>利用梯度下降或者是更高级的算法来实现参数的更新</li>
</ol>
<blockquote>
<p>神经网络的代价函数不是一个凸函数，也是就说他会收敛到一个局部最优值，但这并不影响算法的实际效果。</p>
</blockquote>
</div><div class="tags"><a href="/tags/机器学习"><i class="fa fa-tag">机器学习</i></a></div><div class="post-nav"><a class="pre" href="/2020/05/26/1094-The-Largest-Generation-25%E5%88%86/">1094 The Largest Generation (25分)</a><a class="next" href="/2020/05/25/1093-Count-PAT-s-25%E5%88%86/">1093 Count PAT's (25分)</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://yoursite.com"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="About"><img class="nofancybox" src="/img/avatar.png"/></a><p>To be a better man.</p><a class="info-icon" href="https://twitter.com/username" title="Twitter" target="_blank" style="margin-inline:5px"> <i class="fa fa-twitter-square" style="margin-inline:5px"></i></a><a class="info-icon" href="mailto:admin@domain.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/username" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a><a class="info-icon" href="/atom.xml" title="RSS" target="_blank" style="margin-inline:5px"> <i class="fa fa-rss-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/PAT-Advanced/" style="font-size: 15px;">PAT-Advanced</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/dfs/" style="font-size: 15px;">dfs</a> <a href="/tags/%E4%BA%8C%E5%88%86/" style="font-size: 15px;">二分</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">排序</a> <a href="/tags/%E5%9B%BE%E8%AE%BA/" style="font-size: 15px;">图论</a> <a href="/tags/%E8%BF%9E%E9%80%9A%E5%88%86%E9%87%8F/" style="font-size: 15px;">连通分量</a> <a href="/tags/%E6%A8%A1%E6%8B%9F/" style="font-size: 15px;">模拟</a> <a href="/tags/map/" style="font-size: 15px;">map</a> <a href="/tags/%E9%9B%86%E5%90%88/" style="font-size: 15px;">集合</a> <a href="/tags/%E6%A0%91%E7%9A%84%E7%9B%B4%E5%BE%84/" style="font-size: 15px;">树的直径</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15px;">链表</a> <a href="/tags/%E6%80%9D%E7%BB%B4/" style="font-size: 15px;">思维</a> <a href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" style="font-size: 15px;">动态规划</a> <a href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/" style="font-size: 15px;">字符串</a> <a href="/tags/%E6%99%BA%E5%8A%9B%E9%A2%98/" style="font-size: 15px;">智力题</a> <a href="/tags/%E5%BF%AB%E6%8E%92/" style="font-size: 15px;">快排</a> <a href="/tags/%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">插入排序</a> <a href="/tags/%E5%A0%86/" style="font-size: 15px;">堆</a> <a href="/tags/%E5%B9%B6%E6%9F%A5%E9%9B%86/" style="font-size: 15px;">并查集</a> <a href="/tags/%E4%BC%98%E5%85%88%E9%98%9F%E5%88%97/" style="font-size: 15px;">优先队列</a> <a href="/tags/%E6%95%A3%E5%88%97/" style="font-size: 15px;">散列</a> <a href="/tags/%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">拓扑排序</a> <a href="/tags/STL/" style="font-size: 15px;">STL</a> <a href="/tags/Git/" style="font-size: 15px;">Git</a> <a href="/tags/%E8%A7%84%E8%8C%83/" style="font-size: 15px;">规范</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">机器学习</a> <a href="/tags/%E6%9C%80%E7%9F%AD%E8%B7%AF/" style="font-size: 15px;">最短路</a> <a href="/tags/Dijkstra/" style="font-size: 15px;">Dijkstra</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Tableau%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/" style="font-size: 15px;">Tableau数据分析</a> <a href="/tags/cookie/" style="font-size: 15px;">cookie</a> <a href="/tags/session/" style="font-size: 15px;">session</a> <a href="/tags/UML/" style="font-size: 15px;">UML</a> <a href="/tags/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/" style="font-size: 15px;">软件工程</a> <a href="/tags/servlet/" style="font-size: 15px;">servlet</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" style="font-size: 15px;">数据库</a> <a href="/tags/mongodb/" style="font-size: 15px;">mongodb</a> <a href="/tags/elasticsearch/" style="font-size: 15px;">elasticsearch</a> <a href="/tags/OS/" style="font-size: 15px;">OS</a> <a href="/tags/%E5%B0%8F%E7%8E%A9%E5%85%B7/" style="font-size: 15px;">小玩具</a> <a href="/tags/%E5%9B%9E%E6%BA%AF/" style="font-size: 15px;">回溯</a> <a href="/tags/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/" style="font-size: 15px;">贪心算法</a> <a href="/tags/%E5%8C%BA%E9%97%B4%E9%97%AE%E9%A2%98/" style="font-size: 15px;">区间问题</a> <a href="/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/" style="font-size: 15px;">二叉树</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">深度学习</a> <a href="/tags/CNN/" style="font-size: 15px;">CNN</a> <a href="/tags/%E5%BE%AA%E7%8E%AF%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/" style="font-size: 15px;">循环序列模型</a> <a href="/tags/%E7%99%BD%E7%9B%92%E6%B5%8B%E8%AF%95/" style="font-size: 15px;">白盒测试</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">目标检测</a> <a href="/tags/%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/" style="font-size: 15px;">论文研读</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" style="font-size: 15px;">计算机视觉</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2022/12/24/C-STL%E4%B9%8Bupper-bound%E5%92%8Clower-bound/">C++STL之lower_bound & upper_bound</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/12/14/%E3%80%90OS%E3%80%91%E7%AC%AC%E4%B8%89%E5%A4%A9-%E8%BF%9B%E5%85%A532%E4%BD%8D%E6%A8%A1%E5%BC%8F%E5%B9%B6%E5%AF%BC%E5%85%A5c%E8%AF%AD%E8%A8%80/">【OS】第三天 进入32位模式并导入c语言</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/12/14/%E3%80%90OS%E3%80%91%E7%AC%AC%E4%BA%8C%E5%A4%A9-%E6%B1%87%E7%BC%96%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%E4%B8%8Emakefile%E5%85%A5%E9%97%A8/">【OS】第二天 汇编语言学习与makefile入门</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/12/14/%E3%80%90OS%E3%80%91%E7%AC%AC%E4%B8%80%E5%A4%A9-%E4%BB%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%93%E6%9E%84%E5%88%B0%E6%B1%87%E7%BC%96%E7%A8%8B%E5%BA%8F%E5%85%A5%E9%97%A8/">【OS】第一天 从计算机结构到汇编程序入门</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/29/%E3%80%90%E9%93%BE%E8%A1%A8%E6%80%BB%E7%BB%93%E3%80%91%E5%BF%AB%E6%85%A2%E6%8C%87%E9%92%88%E5%B0%B1%E6%98%AF%E7%89%9B%EF%BD%9E/">【链表总结】快慢指针就是牛～</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/29/%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E9%9D%9E%E9%80%92%E5%BD%92%E9%81%8D%E5%8E%86/">二叉树的非递归遍历</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/25/%E3%80%90%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E3%80%91%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%E4%BA%8C%EF%BC%9A%E5%8C%BA%E9%97%B4%E9%97%AE%E9%A2%98/">【贪心算法】贪心算法系列二：区间问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/25/%E3%80%90%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E3%80%91%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%E4%B8%80/">【贪心算法】贪心算法系列一</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/07/03/%E3%80%90%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95%E3%80%91%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/">【回溯算法】回溯算法总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/06/21/%E3%80%90%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E3%80%91%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%80%BB%E7%BB%93/">【动态规划】动态规划总结</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Links</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2022 <a href="/." rel="nofollow">Xinsteinのblog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="Copy Successed!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>